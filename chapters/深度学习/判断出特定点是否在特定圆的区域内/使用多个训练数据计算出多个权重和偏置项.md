# 使用多个训练数据计算出多个权重和偏置项

> 注明:
>  
- 因为偏导数的正负值可能是错误的 =>梯度下降求最小值的方向可能是错误的。
- 所以这个部分的演示code是有错误的，但是可以从中理解到链式法则迭代中求最小值的深度学习的反式传播算法的内在逻辑。

## 打开实验文件

- 单击右方的[Jupyter Notebook](https://mybinder.org/v2/gh/ipython/ipython-in-depth/master?filepath=binder/Index.ipynb)，稍后在浏览器里会显示Jupyter Notebook的运行环境。
- 在File的第一个下拉菜单“New Notebook” 的右侧箭头处选择“Python 3”，然后会显示一个新的页面
- 把下面的这段python代码拷贝到这个页面“In [ ]:”右侧的空白栏中， 然后单击上方的按键“运行”，然后会显示出相应的图形。

### 简化绝对值函数loss = | - 15w<sub>1</sub> - 5w<sub>2</sub> + b<sub>1</sub> - 10 | 为： loss = 15w<sub>1</sub> + 5w<sub>2</sub> - b<sub>1</sub> + 10 
### 这个问题转化为求函数　y = 15w<sub>1</sub> + 5w<sub>2</sub> - b<sub>1</sub> + 10 接近0的值。 下面就是用梯度下降法通过多种维度找到最接近0的值。

```python
import numpy as np

epsilon = 0.005
w1, w2, b1 = 1, 1, 1
truth = [1,1,1,1,1,1,1,1,0,0]

# 输入第1条数据

v = np.absolute(1/(1+np.exp(-(1*w1+0*w2+b1-truth[0]))))
print("第1条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = 0
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(1*w1+0*w2+b1-truth[0]))))
print("第1条数据点经过梯度下降后的数值是：",v)

# 输入第2条数据

v = np.absolute(1/(1+np.exp(-(0*w1+1*w2+b1-truth[1]))))
print("第2条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = 0 
partial_Differ_Of_w2 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(0*w1+1*w2+b1-truth[1]))))
print("第2条数据点经过梯度下降后的数值是：",v)

# 输入第3条数据

v = np.absolute(1/(1+np.exp(-(-1*w1+0*w2+b1-truth[2]))))
print("第3条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = 0
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(-1*w1+0*w2+b1-truth[2]))))
print("第3条数据点经过梯度下降后的数值是：",v)

# 输入第4条数据

v = np.absolute(1/(1+np.exp(-(0*w1-1*w2+b1-truth[3]))))
print("第4条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = 0
partial_Differ_Of_w2 = np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(0*w1-1*w2+b1-truth[3]))))
print("第4条数据点经过梯度下降后的数值是：",v)

# 输入第5条数据

v = np.absolute(1/(1+np.exp(-(0.5*w1+0.5*w2+b1-truth[4]))))
print("第5条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = - 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = - 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(0.5*w1+0.5*w2+b1-truth[4]))))
print("第5条数据点经过梯度下降后的数值是：",v)

# 输入第6条数据

v = np.absolute(1/(1+np.exp(-(-0.5*w1+0.5*w2+b1-truth[4]))))
print("第6条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = - 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(-0.5*w1+0.5*w2+b1-truth[4]))))
print("第6条数据点经过梯度下降后的数值是：",v)

# 输入第7条数据

v = np.absolute(1/(1+np.exp(-(-0.5*w1-0.5*w2+b1-truth[4]))))
print("第7条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(-0.5*w1-0.5*w2+b1-truth[4]))))
print("第7条数据点经过梯度下降后的数值是：",v)

# 输入第8条数据

v = np.absolute(1/(1+np.exp(-(0.5*w1-0.5*w2+b1-truth[4]))))
print("第8条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = - 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = 0.5 * np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(0.5*w1-0.5*w2+b1-truth[4]))))
print("第8条数据点经过梯度下降后的数值是：",v)

# 输入第9条数据

v = np.absolute(1/(1+np.exp(-(1*w1+1*w2+b1-truth[4]))))
print("第9条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(1*w1+1*w2+b1-truth[4]))))
print("第9条数据点经过梯度下降后的数值是：",v)

# 输入第10条数据

v = np.absolute(1/(1+np.exp(-(-1*w1-1*w2+b1-truth[4]))))
print("第10条数据的输入和输出数值是：",v)

partial_Differ_Of_w1 = np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_w2 = np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。
partial_Differ_Of_b1 = - np.exp(2)/(np.exp(4)+2*np.exp(2)+1) # 偏导计算为负值，但是迭代方向好像不对，改为正号。未来会纠错。

delta_of_w1 = epsilon * partial_Differ_Of_w1
delta_of_w2 = epsilon * partial_Differ_Of_w2
delta_of_b1 = epsilon * partial_Differ_Of_b1 
print(delta_of_w1,delta_of_w2,delta_of_b1)

w1 = w1 + delta_of_w1
w2 = w2 + delta_of_w2
b1 = b1 + delta_of_b1
print(w1,w2,b1)

v = np.absolute(1/(1+np.exp(-(-1*w1-1*w2+b1-truth[4]))))
print("第10条数据点经过梯度下降后的数值是：",v)
```

## 参考文献及资料

1. [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-case-study/)
