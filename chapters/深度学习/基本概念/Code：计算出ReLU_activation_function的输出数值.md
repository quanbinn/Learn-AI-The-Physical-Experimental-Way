# Code：计算出ReLU activation function的输出数值

## 打开实验文件

- 单击右方的[Jupyter Notebook](https://mybinder.org/v2/gh/ipython/ipython-in-depth/master?filepath=binder/Index.ipynb)，稍后在浏览器里会显示Jupyter Notebook的运行环境。
- 在File的第一个下拉菜单“New Notebook” 的右侧箭头处选择“Python 3”，然后会显示一个新的页面
- 把下面的这段python代码拷贝到这个页面“In [ ]:”右侧的空白栏中， 然后单击上方的按键“运行”，然后会显示出相应的图形。

```python

```

## 参考文献及资料

1. [Neural Networks from Scratch](https://nnfs.io/)
2. [Python/p006-Softmax-Activation.py](https://github.com/Sentdex/NNfSiX/blob/master/Python/p006-Softmax-Activation.py)
3. [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-case-study/)